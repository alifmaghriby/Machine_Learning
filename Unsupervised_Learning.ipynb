{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOY9LU/UME7I0EF6NFX9mb/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alifmaghriby/Machine_Learning/blob/main/Unsupervised_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-Means Clustering"
      ],
      "metadata": {
        "id": "9sGi7iuyC3us"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6f7qwbK7CvGf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Load data from Excel\n",
        "data = pd.read_excel(\"your_excel_file.xlsx\")\n",
        "\n",
        "# Assuming 'X1' and 'X2' are the features you want to cluster\n",
        "X = data[['X1_column_name', 'X2_column_name']]  # Use appropriate column names\n",
        "\n",
        "# Create and fit the K-Means model\n",
        "kmeans = KMeans(n_clusters=3)  # Adjust the number of clusters as needed\n",
        "kmeans.fit(X)\n",
        "\n",
        "# Get cluster labels for each data point\n",
        "cluster_labels = kmeans.labels_\n",
        "print(\"K-Means Cluster Labels:\", cluster_labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hierarchical Clustering"
      ],
      "metadata": {
        "id": "c-Bv6rKPC9TC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import scipy.cluster.hierarchy as sch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load data from Excel\n",
        "data = pd.read_excel(\"your_excel_file.xlsx\")\n",
        "\n",
        "# Assuming 'X1' and 'X2' are the features you want to cluster\n",
        "X = data[['X1_column_name', 'X2_column_name']]  # Use appropriate column names\n",
        "\n",
        "# Perform hierarchical clustering\n",
        "dendrogram = sch.dendrogram(sch.linkage(X, method='ward'))\n",
        "\n",
        "# Plot the dendrogram (hierarchical tree)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "1lJr4awxC_B1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Principal Component Analysis (PCA)"
      ],
      "metadata": {
        "id": "d6rF7xOpDAqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load data from Excel\n",
        "data = pd.read_excel(\"your_excel_file.xlsx\")\n",
        "\n",
        "# Assuming 'X1', 'X2', etc., are the features for dimensionality reduction\n",
        "X = data[['X1_column_name', 'X2_column_name', 'X3_column_name']]  # Use appropriate column names\n",
        "\n",
        "# Perform PCA\n",
        "pca = PCA(n_components=2)  # Adjust the number of components as needed\n",
        "X_reduced = pca.fit_transform(X)\n",
        "\n",
        "# X_reduced contains the reduced-dimensional data\n",
        "print(\"PCA Reduced Data:\")\n",
        "print(X_reduced)\n"
      ],
      "metadata": {
        "id": "3Fdkl5ssDCYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "t-SNE (t-Distributed Stochastic Neighbor Embedding)"
      ],
      "metadata": {
        "id": "2YjWAA3LDJ7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Load data from Excel\n",
        "data = pd.read_excel(\"your_excel_file.xlsx\")\n",
        "\n",
        "# Assuming 'X1', 'X2', etc., are the features for dimensionality reduction\n",
        "X = data[['X1_column_name', 'X2_column_name', 'X3_column_name']]  # Use appropriate column names\n",
        "\n",
        "# Perform t-SNE\n",
        "tsne = TSNE(n_components=2)  # Adjust the number of components as needed\n",
        "X_reduced = tsne.fit_transform(X)\n",
        "\n",
        "# X_reduced contains the reduced-dimensional data\n",
        "print(\"t-SNE Reduced Data:\")\n",
        "print(X_reduced)\n"
      ],
      "metadata": {
        "id": "0qmJHJhcDMNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Association Rule Mining (Apriori)"
      ],
      "metadata": {
        "id": "5knBkYfBDOvA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from mlxtend.frequent_patterns import apriori\n",
        "from mlxtend.frequent_patterns import association_rules\n",
        "\n",
        "# Load data from Excel\n",
        "data = pd.read_excel(\"your_excel_file.xlsx\")\n",
        "\n",
        "# Assuming your data is in a transaction format where each row represents items purchased\n",
        "# Example data format:\n",
        "# transaction_id, item_1, item_2, item_3, ...\n",
        "# 1, 'item_a', 'item_b', 'item_c', ...\n",
        "# 2, 'item_a', 'item_d', ...\n",
        "# ...\n",
        "\n",
        "# Convert the data to a one-hot encoded format\n",
        "data_encoded = pd.get_dummies(data.drop('transaction_id', axis=1))\n",
        "\n",
        "# Perform Apriori algorithm\n",
        "frequent_itemsets = apriori(data_encoded, min_support=0.2, use_colnames=True)\n",
        "\n",
        "# Generate association rules\n",
        "association_rules_df = association_rules(frequent_itemsets, metric='lift', min_threshold=1.0)\n",
        "\n",
        "print(\"Association Rules:\")\n",
        "print(association_rules_df)\n"
      ],
      "metadata": {
        "id": "EhpuocmqDQQG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}